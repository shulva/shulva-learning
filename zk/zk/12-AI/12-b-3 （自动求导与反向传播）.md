# 自动求导与反向传播

深度学习框架通过自动计算导数，即**自动微分**（automatic differentiation）来加快求导。 

实际中，根据设计好的模型，系统会构建一个**计算图**（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 

![](../../../files/images/AI/12-b-3-1.png)

自动微分使系统能够随后反向传播梯度。 这里，**反向传播**（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。

### 反向传播

 我们最终的目标是计算**损失函数 L** 相对于网络中**每个参数 $w_i$** 的偏导数 $∂L/∂w_i$

> [!question] 为什么在反向传播中计算梯度？
> 如果要得到一个输出$L$相对于 k 个不同的输入变量$w_i$ 的导数，通常需要进行 k 次独立的前向计算。
> 同理，如果是反向传播，那么m个输出变量就需要m次计算。
> 对于神经网络优化**这种“单输出（损失），多输入（参数）”**的场景，一次正向+一次反向计算的成本要小的多

我们仍然需要正向计算去获得中间值和最终的值来帮助反向传播计算梯度。

![](../../../files/images/AI/12-b-3-3.png)


![](../../../files/images/AI/12-b-3-4.png)

![](../../../files/images/AI/12-b-3-5.png)

![](../../../files/images/AI/12-b-3-6.png)

