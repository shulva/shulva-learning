# 线性回归

_回归_（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法,回归经常用来表示输入和输出之间的关系。

### 线性回归

假设自变量$\mathbf{x}$和因变量$y$之间的关系是线性的![](../../../files/images/AI/12-c-1-1.png)
矩阵同理：
$${\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b$$

$w$权重决定了每个特征对我们预测值的影响。 $b$称为偏置（bias）, 偏置是指当所有特征都取值为0时，预测值应该为多少。

给定一个数据集，我们的目标是寻找模型的权重和偏置， 使得根据模型做出的预测大体符合数据里的真实价格。

严格来说，$\hat{y} = w_1  x_1 + ... + w_d  x_d + b$是输入特征的一个 仿射变换（affine transformation）。 仿射变换的特点是通过加权和对特征进行线性变换（linear transformation）， 并通过偏置项来进行平移（translation）

线性回归的目标是找到一组权重向量$w$和偏置$b$： 当给定从$x$的同分布中取样的新样本特征时， 这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。

在开始寻找最好的模型参数$w$和$b$之前， 我们还需要两个东西： 
（1）一种模型质量的度量方式 。
（2）一种能够更新模型以提高模型预测质量的方法。

### 损失函数

在我们开始考虑如何用模型拟合（fit）数据之前，我们需要确定一个拟合程度的度量

**损失函数**（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。

 回归问题中最常用的损失函数是平方误差函数。 当样本的预测值为$\hat{y}^{(i)}$，其相应的真实标签为$y^{(i)}$时， 平方误差可以定义为以下公式：

$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2$$

常数1/2只是为了求导后常系数为1

为了度量模型在整个数据集上的质量，我们需计算在训练集个样本上的损失均值（也等价于求和）

![](../../../files/images/AI/12-c-2.png)

在训练模型时，我们希望寻找一组参数$\mathbf{w}^*, b^*$， 这组参数能最小化在所有训练样本上的总损失。

> [!question] 为什么是平方误差函数？
> 均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。
> $$y = \mathbf{w}^\top \mathbf{x} + b + \epsilon,\epsilon \sim \mathcal{N}(0, \sigma^2)$$
> $$\epsilon = y-\mathbf{w}^\top \mathbf{x} - b$$
>$$P(\epsilon) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{\epsilon ^2}{2 \sigma^2}\right).$$
> 如上可知$y与\epsilon 一一映射$，固定$\mathbf{x}$时，$y$的随机性由$\epsilon$决定，故有$$P(y \mid \mathbf{x}) = P(\epsilon)$$
> 则有：$$P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).$$
> 
> 根据极大似然估计法，参数w和b的最优值是使整个数据集的**似然**最大的值:$$P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).$$
> (使这个概率和最大，使得$\mathbf{y}$出现概率最大的那个参数值$\mathbf{X}$就是最合理的值)
> 
> 根据最大似然估计，我们最大化，相当于最小化负对数似然。
> 取最小化负似然对数方便计算，则有:$$-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n (\frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2)$$
>第一项不依赖于w和b，第二项与均方误差形式相同
> **平方误差损失函数背后隐含了一个数据噪声服从高斯分布的假设**

### 解析解

与我们将在本书中所讲到的其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）。

将损失关于$\mathbf{w}$的导数设为0，得到解析解：

![](../../../files/images/AI/12-c-3.png)

像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

### 线性回归到神经网络

我们依然可以用描述神经网络的方式来描述线性模型， 从而把线性模型看作一个神经网络。

![](https://zh-v2.d2l.ai/_images/singleneuron.svg)

如图所示的神经网络中，输入为$x_1, \ldots, x_d$， 因此输入层中的**输入数**（或称为**特征维度**，feature dimensionality）为$d$。 网络的输出为$o_1$，因此输出层中的**输出数**是1。

由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。 也就是说，图中神经网络的层数为1。

对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换（图中的输出层） 称为**全连接层**（fully-connected layer）或称为**稠密层**（dense layer）。

