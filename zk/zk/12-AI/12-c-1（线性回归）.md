# 线性回归

_回归_（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法,回归经常用来表示输入和输出之间的关系。

### 线性回归[^1]

假设自变量$\mathbf{x}$和因变量$y$之间的关系是线性的![](../../../files/images/AI/12-c-1-1.png)
矩阵同理：
$${\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b$$

$w$权重决定了每个特征对我们预测值的影响。 $b$称为偏置（bias）, 偏置是指当所有特征都取值为0时，预测值应该为多少。

给定一个数据集，我们的目标是寻找模型的权重和偏置， 使得根据模型做出的预测大体符合数据里的真实价格。

严格来说，$\hat{y} = w_1  x_1 + ... + w_d  x_d + b$是输入特征的一个 仿射变换（affine transformation）。 仿射变换的特点是通过加权和对特征进行线性变换（linear transformation）， 并通过偏置项来进行平移（translation）

线性回归的目标是找到一组权重向量$w$和偏置$b$： 当给定从$x$的同分布中取样的新样本特征时， 这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。

在开始寻找最好的模型参数$w$和$b$之前， 我们还需要两个东西： 
（1）一种模型质量的度量方式 。
（2）一种能够更新模型以提高模型预测质量的方法。

### 损失函数[^2]

在我们开始考虑如何用模型拟合（fit）数据之前，我们需要确定一个拟合程度的度量

**损失函数**（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。

 回归问题中最常用的损失函数是平方误差函数。 当样本的预测值为$\hat{y}^{(i)}$，其相应的真实标签为$y^{(i)}$时， 平方误差可以定义为以下公式：

$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2$$

常数1/2只是为了求导后常系数为1

为了度量模型在整个数据集上的质量，我们需计算在训练集个样本上的损失均值（也等价于求和）

![](../../../files/images/AI/12-c-2.png)

在训练模型时，我们希望寻找一组参数$\mathbf{w}^*, b^*$， 这组参数能最小化在所有训练样本上的总损失。

> [!question] 为什么是平方误差函数？[^5]
> 均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。
> $$y = \mathbf{w}^\top \mathbf{x} + b + \epsilon,\epsilon \sim \mathcal{N}(0, \sigma^2)$$
> $$\epsilon = y-\mathbf{w}^\top \mathbf{x} - b$$
>$$P(\epsilon) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{\epsilon ^2}{2 \sigma^2}\right).$$
> 如上可知$y与\epsilon一一映射$，固定$\mathbf{x}$时，$y$的随机性由$\epsilon$决定，故有$$P(y \mid \mathbf{x}) = P(\epsilon)$$
> 则有：$$P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).$$
> 
> 根据极大似然估计法，参数w和b的最优值是使整个数据集的**似然**最大的值:$$P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).$$
> (使这个概率和最大，使得$\mathbf{y}$出现概率最大的那个参数值$\mathbf{X}$就是最合理的值)
> 
> 根据最大似然估计，我们最大化，相当于最小化负对数似然。
> 取最小化负似然对数方便计算，则有:$$-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n (\frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2)$$
>第一项不依赖于w和b，第二项与均方误差形式相同
> **平方误差损失函数背后隐含了一个数据噪声服从高斯分布的假设**

### 解析解

与我们将在本书中所讲到的其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）。

将损失关于$\mathbf{w}$的导数设为0，得到解析解：

![](../../../files/images/AI/12-c-3.png)

像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

### 随机梯度下降

得不到解析解的情况下我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。

**梯度下降**（gradient descent）几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。

梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。

![](../../../files/images/AI/12-c-5.png)
 
 但梯度下降实际执行可能会非常慢，因为在一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候只随机抽取一小批样本， 这种叫做**小批量随机梯度下降**。

在每次迭代中，我们首先随机抽样一个小批量$\mathcal{B}$。 然后，我们计算小批量的梯度。最后，我们将梯度乘以一个预先确定的正数$\eta$(学习率），并从当前参数的值中减掉(使损失函数值变小的方向）。[^4]

![](../../../files/images/AI/12-c-6.png)

表现为公式则有：

$$\begin{split}\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\end{split}$$

$|\mathcal{B}|$表示每个小批量中的样本数，这也称为批量大小（batch size）。

批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。 这些可以调整但不在训练过程中更新的参数称为**超参数**（hyperparameter）。 

**调参**（hyperparameter tuning）是选择超参数的过程。 超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的验证数据集上评估得到的。

![](../../../files/images/AI/12-c-7.png)

给定特征估计/预测目标的过程通常称为**推理**（inference）。

### 线性回归到神经网络[^6]

我们依然可以用描述神经网络的方式来描述线性模型， 从而把线性模型看作一个神经网络。

![](https://zh-v2.d2l.ai/_images/singleneuron.svg)

如图所示的神经网络中，输入为$x_1, \ldots, x_d$， 因此输入层中的**输入数**（或称为**特征维度**，feature dimensionality）为$d$。 网络的输出为$o_1$，因此输出层中的**输出数**是1。

由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。 也就是说，图中神经网络的层数为1。

对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换（图中的输出层） 称为**全连接层**（fully-connected layer）或称为**稠密层**（dense layer）。

神经网络还有一些有趣的，与生物学相关的起源。[^7]

### 矢量化加速[^9]

在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 
为了实现这一点，需要我们对计算进行**矢量化**， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。

为了说明矢量化为什么如此重要，我们考虑对向量相加的两种方法。 
我们实例化两个全为1的10000维向量。 

在一种方法中，我们将使用Python的for循环遍历向量。 
在另一种方法中，我们将依赖对`+`的调用。

```python

n = 10000
a = torch.ones([n])
b = torch.ones([n])

c = torch.zeros(n)
timer = Timer()
for i in range(n):
    c[i] = a[i] + b[i] #'0.16749 sec'

d = a + b # '0.00042 sec'

```

 矢量化代码通常会带来数量级的加速。 另外，我们将更多的数学运算放到库中，而无须自己编写那么多的计算，从而减少了出错的可能性。


[^1]: https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html#id8

[^2]: https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html#id4

[^4]: https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html#id6

[^5]: https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html#subsec-normal-distribution-and-squared-loss

[^6]: https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html#id10

[^7]: https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html#id12

[^8]: https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html#subsec-linear-model

[^9]: https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html#id8
