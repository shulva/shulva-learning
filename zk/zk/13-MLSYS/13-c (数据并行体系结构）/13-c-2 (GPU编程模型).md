# GPU

![](../../../../files/images/MLsys/13-c/13-b-2-2.png)
![](../../../../files/images/MLsys/13-c/13-b-2-24.png)

![](../../../../files/images/MLsys/13-c/13-b-2-23.png)

### GPU的编程模型

![](../../../../files/images/MLsys/13-a/13-b-3.jpg)

指令流水线类似于SIMD的流水线,GPU前端编程模型不是用SIMD指令编程。其的编程模型以线程作为抽象的单位，每个线程执行同样的代码，但操作不同的数据元素。每个线程有自己的上下文(即可以独立地启动/执行等）。

一组执行相同指令的线程由硬件动态组织成**线程组warp**，一个warp由硬件形成的SIMD操作来实现。

本质上是一个[SPMD](13-c-1%20(并行方式).md#SPMD与SIMT)的模型。GPU并不会去独立地管理和调度成千上万个线程，那样控制开销太大。GPU会将线程打包成固定大小的组warp，以此作为基本的GPU调度单位。
![](../../../../files/images/MLsys/13-c/13-b-2-3.png)

> [!question] 什么是标量指令流 (Scalar Instruction Stream)？
> 我们从一个程序员的角度来看。当你编写CUDA核函数时，你写的代码是针对**一个线程**的。
> 
> ```cpp
> __global__ void my_kernel(float* a, float* b, float* c) {
>     int i = threadIdx.x;
>     
>     // 以下就是一个线程的指令流
>     float val_a = a[i];   // 指令1: 加载
>     float val_b = b[i];   // 指令2: 加载
>     c[i] = val_a + val_b; // 指令3: 加法; 指令4: 存储
> }
> ```
> 
> 在这个例子中，一个线程执行的指令序列就是：
> 1. 从全局内存 a 中加载一个浮点数。
> 2. 从全局内存 b 中加载一个浮点数。
> 3. 将这两个数相加。
> 4. 将结果存回全局内存 c。
> 
> 这个指令序列就是**一个线程的指令流**。之所以称之为“**标量 (Scalar)**”，是因为每条指令处理的都是单个数据（比如一个 float），而不是一个向量或数据集合。这与CPU中的SIMD（Single Instruction, Multiple Data）指令（如AVX指令）形成了对比。在CUDA编程模型中，你写的是操作标量数据的代码。

![](../../../../files/images/MLsys/13-c/13-b-2-4.png)

![](../../../../files/images/MLsys/13-c/13-b-2-5.png)

![](../../../../files/images/MLsys/13-c/13-b-2-6.png)

> [!example] 一个简化的CUDA示例：DAXPY loop
> 计算由大量的相互独立的线程(CUDA threads or microthreads) 完成。
> DAXPY loop 指的是完成 **Y = a*X + Y** 这个向量运算的循环。
> 
> ```cpp
> // C version of DAXPY loop. //n代表向量x和y中元素的数量 
> void daxpy(int n, double a, double*x, double*y) 
> { 
> 	for (int i=0; i<n; i++) 
> 	y[i] = a*x[i] + y[i];
> }
> 
> // CUDA version. 
> __host__ // Piece run on host processor. 
> int nblocks = (n+255)/256; // 256 CUDA threads/block 
> daxpy<<<nblocks,256>>>(n,2.0,x,y); 
> 
> __device__ // Piece run on GP-GPU. 
> void daxpy(int n, double a, double*x, double*y) 
> { 
> 	int i = blockIdx.x*blockDim.x + threadId.x; 
> 	if (i<n) 
> 		y[i]=a*x[i]+y[i]; 
> }
> 
> ```

### GPU/SIMT中的控制流问题

在基于 Warp 的 SIMD 中，线程可以走不同的路径。每个线程可以包含控制流指令，这些线程可以执行不同的控制流路径。

![](../../../../files/images/MLsys/13-c/13-b-2-8.png)

分支分歧(Branch divergence)例如:
```cpp
	if (threadId < 16) condition A;
	else condition B;
```

一个warp中的所有线程不能在同一时刻执行相同的指令会影响性能。

![](../../../../files/images/MLsys/13-c/13-b-2-7.png)

路径一致时，只需要执行唯一的一条路径，速度快效率高。

路径不一致时的处理则如下：
- 创建**屏蔽向量 (Mask)**：如果线程们选择了不同的方向，硬件会立刻创建一个32位的“屏蔽向量”（Mask）。Mask的每一位对应一个线程，1 代表它要走A路径（比如 if），0 代表它要走B路径（比如 else）。
- 选择一条路，另一条路“压栈”：硬件会选择一条路径先走（比如 else 路径）。然后，它会把另一条路径（if 路径）的**入口地址**和对应的**屏蔽字 (Mask)** 一起**压入一个专用的硬件堆栈**里。这个堆栈就是PPT里提到的“**分支同步堆栈 (SIMT-Stack)**”。
- **串行化执行**： ^13-c-2-1
    1. Warp开始执行被选中的第一条路径（else）。此时，只有那些要去 else 路径的线程是**活动的 (active)**，其他要去 if 路径的线程则被**屏蔽 (masked off)**，原地等待，什么也不做。
    2. 当所有活动线程走完 else 路径后，硬件检测到这条路走完了。走不同的路径会导致效率变低。

 何时执行堆栈里“待办”的路径？
- 出栈 (Pop) 并执行：
    - 当第一条路径执行完毕，硬件会自动检查分支同步堆栈。
    - 它发现堆栈里还有一个“待办事项”（之前压入的 if 路径）。于是硬件执行 **Pop 操作**，弹出 if 路径的入口地址和屏蔽字。
    - 现在，Warp开始执行 if 路径的代码。这一次，之前等待的线程变为**活动**状态，而刚刚走完 else 路径的线程则被**屏蔽**，开始等待。
- **汇聚 (Converge)**：
    - 当 if 路径也执行完毕后，硬件再次检查堆栈，发现堆栈空了。
    - 这意味着这个 if-else 分支的所有可能路径都已被Warp遍历。
    - 此时，所有32个线程在 if-else 结构之后的**第一条共同指令处重新汇合**，恢复到完全同步的SIMD执行模式。这个重新同步的点被称为**汇聚点 (Convergence Point)**。


![](../../../../files/images/MLsys/13-c/13-b-2-9.png)
答案是不能，硬件的物理设计是固定的。假设我们想把lane0上的线程0移动到lane1上，我们会发现：
1. 数据的物理位置：线程0 的所有数据（它的上下文、变量）都物理存储在**第一个寄存器堆**里。
2. 硬件的连接方式：**Lane 1** 的功能单元在物理上是和**第二个寄存器堆**连接的。它只能读取和写入第二个寄存器堆的数据。
3. **移动的代价**：要想让Lane 1执行线程0的任务，就必须把线程0的所有寄存器数据从第一个寄存器堆**物理地移动**到第二个寄存器堆。这需要在所有寄存器堆之间建立一个极其复杂和庞大的交叉开关网络（Crossbar Switch）。
4. **设计的权衡**：建立这样的交叉开关网络会消耗巨大的芯片面积、功耗，并增加访问延迟。为了追求极致的并行计算效率和能效比，GPU设计师选择了放弃了这种灵活性。他们采用了图中所示的**静态映射**（Static Mapping）方案：
    - Thread ID % WarpSize（在这里是 Thread ID % 4）决定了这个线程被固定在哪一个Lane上执行。
    - Thread 0 永远在 Lane 0 上执行。
    - Thread 1 永远在 Lane 1 上执行。
    - Thread 5 (因为 5 % 4 = 1) 也永远在 Lane 1 上执行。

这种方式叫做静态线程分组。在程序开始运行之前，线程如何被分组（哪个线程属于哪个Warp）就已经固定下来了。threadIdx, blockIdx 在启动时就唯一确定了一个线程的ID，它所属的Warp（由threadIdx计算得出）也是固定的。

动态线程分组固然有他的好处，其理论上可以提高计算单元的利用率。例如，当两个Warp都因为分支分歧而只有部分线程活跃时，可以把这两个Warp中的活跃线程抽出来，组成一个新的Warp，从而避免计算单元闲置。

但是事实上，这种方式在访存端会有严重的问题。这会导致三个问题：
1. **使得访问模式具有随机性**
    - 核心原因。在静态分组下，一个Warp内的线程（如线程0到31）通常被设计来访问**连续的内存地址**（例如，`A[0]-A[31]`）。这种规整的、可预测的访问模式对存储系统非常友好。
    - 但如果动态地把线程0、线程35、线程99、线程120...这些原本毫不相关的线程组合成一个新的Warp，那么它们发出的内存访问请求将会指向内存中**完全不相邻、随机分布**的位置。
2. **导致存储器访问的局部性变得困难**
    - **直接后果**:存储系统的性能极大依赖于**局部性原理**（特别是空间局部性）。当一个Wapr请求一片连续内存时，DRAM可以一次性高效地获取整个缓存行（Cache Line）的数据，服务于所有线程。
    - 而随机的访问模式完全破坏了空间局部性。Warp中的32个线程可能需要访问32个完全不同的缓存行，这会触发多次独立的、低效的DRAM访问。
3. **导致存储器带宽利用率的下降**
    - GPU拥有极高的存储带宽，但要充分利用它，前提是进行[合并访存](https://firstmoonlight.github.io/2025-03-30-%E5%90%88%E5%B9%B6%E8%AE%BF%E5%AD%98/),即一个Warp的访问请求能被合并成少数几个大的DRAM事务。随机的、非合并的访问会导致大量的带宽浪费。

![](../../../../files/images/MLsys/13-c/13-b-2-10.png)

Problem: 一个Warp中有些命中(hit)，有些不命中(miss)
    - 假设线程0要的数据恰好在L1 Cache里（Hit），而线程1要的数据不在Cache里，需要去访问遥远且缓慢的DRAM（Miss，非常慢）。
    - 因为Warp是一个**执行单元**，必须一同执行。即使线程0已经拿到了自己的数据，它也不能继续往下执行，必须停下来，**等待**线程1的数据从DRAM中取回。**整个Warp的执行速度被最慢的那个内存访问给拖累了。**
Problem: 一个线程因为访存行为而阻塞停止，导致整个Warp都要停顿
    - **这是对SIMT/SIMD执行模型的再次强调**。Warp作为一个整体被调度。只要Warp中**有任何一个线程**因为等待内存数据而**阻塞 (Stall)**，那么**整个Warp的所有32个线程都会被挂起**，从执行流水线上换下来。造成计算资源的浪费。

### SIMD vs SIMT

![](../../../../files/images/MLsys/13-c/13-b-2-11.png)

| 特性       | SIMD (Single Instruction, Multiple Data)                           | SIMT (Single Instruction, Multiple Threads)                                                                                                                         |
| -------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **核心思想** | 直接作用于计算执行单元或向量单元                                                   | 将并行概念扩展到利用线程                                                                                                                                                        |
| **指令流**  | 一条指令流（一串顺序的SIMD指令）,每条指令对应多个数据输入（向量指令）                              | 多个指令流（标量指令）构成线程，这些线程动态构成Warp。一个Warp处理多个数据元素                                                                                                                         |
| **编程模型** | 程序员需要显式地编写向量代码（例如，使用Intel AVX intrinsics），思考如何将数据打包成向量             | 程序员只需编写**单个线程**的标量代码，硬件自动将多个线程捆绑成Warp来并行执行                                                                                                                          |
| **主要优点** | 1. **提高了数据级并行性**提高了数据级并行性（而不是指令级或并发性）。<br>2. 使CPU能够对不同的操作数执行相同的任务。 | 1. **编程模型更灵活**：逻辑上可以独立地处理线程（类似MIMD），每个线程可以有自己的控制流（如if-else）。<br>2. **硬件执行高效**：可以将执行相同指令流的线程组织成Warp，充分发挥SIMD处理的硬件优势。<br>3. **有效隐藏延迟**：通过在Warp间切换，减少了与指令预取和内存访问相关的延迟。 |
| **抽象层次** | **较低**：更贴近硬件，直接操作数据向量。                                             | **较高**：对程序员隐藏了向量化的复杂性，提供了更通用的线程抽象。                                                                                                                                  |
SIMT在抽象层次上高于SIMD，因为它扩展了SIMD的概念，将其应用于多个线程，而不仅仅是单个数据集

![](../../../../files/images/MLsys/13-c/13-b-2-12.png)
### 线程组Warp

![](../../../../files/images/MLsys/13-c/13-b-2-13.png)

![](../../../../files/images/MLsys/13-c/13-b-2-14.png)
Warp调度器会切换Warp。当WarpA在等待读取数据时（这个过程很慢），调度器可以立刻切换到WarpB去执行计算指令，从而让计算单元始终保持忙碌。

![](../../../../files/images/MLsys/13-c/13-b-2-15.png)
严格来说，下图最后一句话的叙述应当为：多个线程块，可以**同时**驻留在同一个**SM**上。多个线程块的**线程**，在**不同时间**，可以在同一个 **Core** 上运行。线程块的驻留由全局硬件调度器管理。

![](../../../../files/images/MLsys/13-c/13-b-2-16.png)
warp的指令级并行让不同的warp在不同的功能单元上运作，提高整体的利用率。

![](../../../../files/images/MLsys/13-c/13-b-2-17.png)

Warp可以通过调度的方式隐藏一些大的延迟（访存之类）

![](../../../../files/images/MLsys/13-c/13-b-2-18.png)

> [!NOTE] 解释
> - SIMD Pipeline (SIMD流水线): 代表SM的执行引擎，包含取指、解码、执行（ALU）、访存（D-Cache）、写回等阶段。
> - **Warps available for scheduling (可供调度的Warp池)**:
>     - 这里面是准备就绪的Warp，比如Thread Warp 3, 8, 7。
>     - **准备就绪**意味着它们的下一条指令是计算指令，或者它们需要的内存数据已经成功取回。
>     - Warp调度器会从这个池子中挑选一个Warp，将其下一条指令送入SIMD流水线的入口（I-Fetch）。
> - **Warps accessing memory hierarchy (正在访问内存的Warp池)**:
>     - 这里面是被阻塞或等待中的Warp，比如Thread Warp 1, 2, 6。
>     - **关键流程**:
>         1. 调度器选择了**Warp1**执行。Warp1的指令进入流水线。
>         2. 当指令执行到D-Cache（数据缓存）阶段时，它需要从内存加载一个数据。
>         3. 硬件检查缓存，发生了“Miss?”（缓存未命中）。这意味着需要去访问缓慢的主内存。
>         4. 此时，**Warp 1被立刻移出执行流水线，并被放入“等待内存”的池子中**。
>         5. 流水线并不会因此停顿！在下一个时钟周期，Warp调度器会立即从“可供调度”的池子中挑选另一个Warp（比如Warp3），送入流水线开始执行。
>         6. 就这样，流水线始终被“准备就绪”的Warp所填充，保持着极高的利用率。
>         7. 过了很久（几百个周期后），Warp 1所需要的数据从主内存返回（如图中Data箭头所示）。此时，Warp 1的状态就从“等待中”变为了“准备就绪”，它会被移回到“可供调度”的池子中，等待下一次被调度执行的机会。
> 

