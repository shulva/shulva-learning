# 并行的两种方式

![](../../../../files/images/MLsys/13-b/13-b-1-4.png)
### 多核与众核

更简单的内核：每个内核在运行单个指令流时都比我们原来的Fancy 内核慢 (e.g., 25% slower) 。但是现在有两个核心: 2 × 0.75 = 1.5 (potential for speedup!)。在并行的程序上，多核无疑表现的更好

![](../../../../files/images/MLsys/13-b/13-b-1-5.png)


![](../../../../files/images/MLsys/13-b/13-b-1-21.png)
### SIMD

![](../../../../files/images/MLsys/13-b/13-b-1-2.png)

![](../../../../files/images/MLsys/13-b/13-b-1-6.png)

例如，我们可以使用 256 位向量寄存器上的向量指令同时处理8个数组元素

```
vloadps xmm0, addr[r1] 
vmulps xmm1, xmm0, xmm0 
vmulps xmm1, xmm1, xmm0
...
vstoreps addr[xmm2], xmm0
```

![](../../../../files/images/MLsys/13-b/13-b-1-7.png)

如果遇到条件执行的语句会怎样？在这种情况下，ALU会屏蔽（丢弃）输出，并不是所有的 ALU 都能做有用的工作。最坏情况下只会有1/8的peak performance。

![](../../../../files/images/MLsys/13-b/13-b-1-8.png)

在cuda中，你也可以见到[分支发散](../13-a%20(CUDA)/CUDA编程/13-a-6（线程束Warp）.md#SM与SIMT)的情况。

![](../../../../files/images/MLsys/13-b/13-b-1-9.png)
#### SIMD-向量处理器

向量处理器的根本工作模式类似于计算 C = A + B，其中A、B、C都是向量（数组），向量处理器会执行 C[i] = A[i] + B[i]，对所有 i 都成立。

![](../../../../files/images/MLsys/13-b/13-b-1-10.png)

> [!NOTE] 解释
> 1. fewer instruction fetches意味着更少的指令读取，因为一条向量指令（如 VADD）替代了标量处理器中的一个完整循环（包括加载数据、计算、存储结果、增加循环计数、判断循环结束、跳转等一系列指令），处理器需要从内存中读取的指令数量大大减少。**不需要频繁地去取指，降低了指令带宽压力。译码一次，执行多次操作，减少了指令译码的开销**
> 2. 独立性：计算一个元素的结果（如 C[i]）不依赖于前一个元素的结果（如 C[i-1]）。数据相互独立，不需要考虑流水线中的转发/旁路，从而可以设计长流水线。这样可以得到更高的时钟频率，而且硬件只需检测两条向量指令间的相关性，内部的相关性不需要考虑。
> 3. 向量指令通常需要**访问内存中一片连续或有规律间隔的数据**，这种可预测的内存访问模式带来了巨大的性能优势。多体交叉存储器（Interleaved Memory）将内存分成多个独立的模块（Bank）。因为向量访问模式是可预测的（例如，连续访问 A[0], A[1], A[2]...），处理器可以提前向不同的Bank同时发出请求，实现并行访存，极大地提高了内存带宽。
> 4. 可通过重叠减少存储器操作的延时：这里的“重叠”指的是内存访问和计算操作的重叠（流水线化）。当第一个数据元素从内存加载后开始计算时，硬件已经开始加载第二个数据元素了。这样，很长的内存访问延迟被隐藏起来。对整个向量来说，平均到每个元素上的访存延迟就非常低了。
> 5. 仅使用指令cache：因为向量访存是高度规律的“流式”访问，数据几乎不会被重复使用，所以传统的缓存（Cache）机制（利用数据的局部性原理）效果不佳。它们转而使用高带宽的交叉内存来直接满足数据流的需求。不过，现代的很多向量处理器也会集成数据缓存。
> 6. 控制相关主要由程序中的分支和循环指令引起。向量指令将整个循环封装在一条指令内，从而消除了循环控制本身带来的分支指令。这使得处理器流水线可以不需要进行分支预测，也避免了因预测失败而导致的流水线冲刷（Pipeline Flush）惩罚，执行效率极高。

向量处理器主要有**memory-memory vector processors**，所有的向量操作是内存到内存以及**vector-register processors**，除了load 和store操作外，所有的操作是向量寄存器与向量寄存器间的操作。

![](../../../../files/images/MLsys/13-b/13-b-1-11.png)

访存导致**memory-memory vector processors**存在一定的劣势。

![](../../../../files/images/MLsys/13-b/13-b-1-12.png)
##### 向量运算过程

![](../../../../files/images/MLsys/13-b/13-b-1-13.png)

两条向量指令之间还是有相关性的，这也是dead time出现的原因。

![](../../../../files/images/MLsys/13-b/13-b-1-14.png)


关于向量处理器的其他内容，参考提供的ppt即可。
#### SIMD-Array Processor
![](../../../../files/images/MLsys/13-b/13-b-1-15.png)

> [!question] VLIW与SIMD的区别
> VLIW 的目标和实现方式与 SIMD 完全不同。VLIW 是一种**指令级并行 (Instruction-Level Parallelism, ILP)** 技术。VLIW将多个独立的操作由编译器封装在一起，在一个 VLIW 指令包中，各个功能单元执行的**操作可以是完全不同的**。例如，同一个时钟周期内，一个单元在做整数加法，另一个在做浮点乘法，第三个在从内存读取数据。这违背了 SIMD 的“Single Instruction”原则。例如，Array processor:便是**单个操作**作用在多个不同的数据元素上。

#### Packed SIMD

![](../../../../files/images/MLsys/13-b-1-51.png)

![](../../../../files/images/MLsys/13-b/13-b-1-17.png)
### SPMD与SIMT

引子：**如何挖掘和发挥程序中潜在的并行性？**

```cpp
for(int i=0;i<N;i++)
	c[i] = a[i]+b[i];
```

下面，让我们来看看如何利用三种不同的编程模型，来充分发挥这段顺序代码中潜在的指令级并行性：
1. 顺序（SISD)
2. 数据并行（SIMD）
3. 多线程（MIMD/SPMD）

###### Prog. Model 1: 顺序(SISD)

![](../../../../files/images/MLsys/13-b/13-b-1-18.png)
###### Prog. Model 2: 数据并行(SIMD)

![](../../../../files/images/MLsys/13-b/13-b-1-19.png)

![](../../../../files/images/MLsys/13-b/13-b-1-20.png)

这种特定模型也称为**单程序多数据(SPMD, Single Program Multiple Data)**，可以在**单指令多线程SIMT**上执行。

SPMD是一种编程模型,并不是一种硬件架构。所有处理单元（或进程、线程）都执行同一份程序代码的副本，但每个处理器操作的是各自不同的数据。虽然所有处理器运行的是同一个程序，但它们可以在运行时执行不同的控制流路径。

**SIMD是每个时刻所有处理单元执行同一条指令，SPMD是所有处理单元执行同一段代码，但不必在同一时刻执行同一个指令。**

Programming Model 是软件的概念，指程序员如何描述应用。从程序员角度看到的机器模型有很多，例如,顺序模型 (von Neumann), 数据并行(SIMD), 数据流模型、多线程模型 (MIMD, SPMD), …

Execution Model是硬件的概念，指硬件底层如何执行代码。例如,乱序执行、向量机、数据流处理机、多处理机、多线程处理机等。执行模型与编程模型可以差别很大。例如,顺序模型可以在乱序执行的处理器上执行，[SPMD](13-c-1%20(并行方式).md#SPMD与SIMT)模型可以用SIMD处理器实现 (GPU)。

许多科学计算应用以SPMD这种方式编程，运行在MIMD硬件结构上 (multiprocessors) ，现代GPU以这种类似的方式编程，运行在SIMD硬件上。

关于SIMD,SPMD,SIMT的内容可参考[SIMD vs SIMT](13-c-2%20(GPU编程模型).md#SIMD%20vs%20SIMT)

