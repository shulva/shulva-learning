# 共享内存

共享内存是一种可被程序员直接操控的缓存，主要作用有两个：一个是减少核函数中对全局内存的访问次数，实现高效的线程块内部的通信，另一个是提高全局内存访问的合并度。
![](../../../../../files/images/MLsys/13-a/13-a-4-1-3.png)

![](../../../../../files/images/MLsys/13-a/13-a-4-1-1.png)

我们将通过数组规约和矩阵转置两个例子来学习共享内存的使用方法。
### 数组规约-cpu

考虑一个有N个元素的数组x，假如我们需要计算该数组中所有元素的和，即sum = x[0] + x[1] + ... + x[N - 1]。

```cpp
real reduce(const real *x, const int N)
{
    real sum = 0.0;
    for (int n = 0; n < N; ++n)
    {
        sum += x[n];
    }
    return sum;
}
```

如果使用单精度浮点数，会出现`sum = 33554432.000000`这样完全错误的结果。这是因为，在累加计算中出现了所谓的“大数吃小数”的现象。单精度浮点数只有 6、7 位精确的有效数字。在上面的函数 reduce 中，将变量 sum 的值累加到 3000 多万后，再将它和 1.23 相加，其值就不再增加了（小数被大数“吃掉了”，但大数并没有变化）。

---

### 数组规约-全局内存

对于数组归约的并行计算问题，我们是要从一个数组出发，最终得到一个数。所以，必须使用某种迭代方案。

假如数组元素个数是 2 的整数次方（我们稍后会去掉这个假设），我们可以将数组后半部分的各个元素与前半部分对应的数组元素相加。

如果重复此过程，最后得到的第一个数组元素就是最初的数组中各个元素的和。这就是所谓的折半归约（binary reduction）法。

```cpp
void __global__ reduce_global(real *d_x, real *d_y)
{
    const int tid = threadIdx.x;
    real *x = d_x + blockDim.x * blockIdx.x;

    for (int offset = blockDim.x >> 1;offset > 0;offset >>= 1)
    {
        if (tid < offset)
        {
            x[tid] += x[tid + offset];
        }
        __syncthreads();//同步机制防止顺序不正确
    }

    if (tid == 0)
    {
        d_y[blockIdx.x] = x[0];//host再规约剩下的
    }
}
```

为了方便分析，我们将上述核函数中循环的前两次迭代明显地写出来：
```cpp
// N = blockDim.x
if (n < N / 2) { d_x[n] += d_x[n + N / 2]; } 
if (n < N / 4) { d_x[n] += d_x[n + N / 4]; }
```

考察对数组元素 `d_x[N / 4]` 的操作。

在第一次迭代中会出现向数组元素 `d_x[N / 4]` 写入数据的操作（由线程 `n = N / 4` 执行）；
在第二次迭代中会有出现 `d_x[N / 4]` 取出数据的操作（由线程 `n = 0` 执行）。有一种可能的情况是： 在线程 `n = 0` 开始执行第二行语句时，线程 `n = N / 4` 还没执行完第一行语句。如果这种情况发生了，就有可能得到预料之外的结果。

> [!question] 同步
> 
> 要保证核函数中语句的执行顺序与出现顺序一致，就必须使用某种同步机制。 在 CUDA 中，提供了一个同步函数 `__syncthreads()`。该函数只能用在核函数中。
> 
> 至于两个不同线程块中的线程，则不一定按照代码出现的顺序执行指令，但这不影响程序的正确性。这是因为，在该核函数中，每个线程块都处理不同的数据，相互之间没有**数据依赖**。总结起来就是说：一个线程块内的线程需要合作，所以需要同步；两个线程块之间不需要合作，所以不需要同步。(因为循环以`offset=blockDim.x>>1`，即线程块大小为基础)
> 
> 当计算被完全限制在单个Warp的内部时，我们就不再需要使用 `__syncthreads()` 这一指令来进行显式同步了，因为Warp本身的硬件执行模型已经提供了隐式的同步保证。
> 
![](../../../../../files/images/MLsys/13-a/13-a-4-1-2.png)
### 数组规约-共享内存

我们注意到，在前一个版本的核函数中，对全局内存的访问是很频繁的。我们介绍过， 全局内存的访问速度是所有内存中最低的，应该尽量减少对它的使用。所有设备内存中，寄存器是最高效的，但在需要线程合作的问题中，用仅对单个线程可见的寄存器是不够的。我们需要使用**对整个线程块可见的共享内存**。

在核函数中，要将一个变量定义为共享内存变量，就要在定义语句中加上一个限定符 `__shared__`,我们可以定义如下共享内存数组变量：`__shared__ real s_y[128];`

如果没有限定符` __shared__`，该语句将极有可能定义一个长度为 128 的局部数组。

要强调的是，在一个核函数中定义一个共享内存变量，就相当于在每一个线程块中有了一个该变量的副本。每个副本都不一样，虽然它们共用一个变量名。

核函数中对共享内存变量的操作都是同时作用在所有的副本上的。当然，一个**线程块**对共享内存的任何操作都**不会影响到其他线程块**的共享内存。

```cpp
void __global__ reduce_shared(real *d_x, real *d_y)
{
    const int tid = threadIdx.x;
    const int bid = blockIdx.x;
    const int n = bid * blockDim.x + tid;
	
    __shared__ real s_y[128];
    
	// bid = 0,第零个线程块的s_y[] = d_x[0 ~ blockDim.x-1]
	// bid = 1,第一个线程块的s_y[] = d_x[blockDim.x ~ 2 *blockDimx-1]
    s_y[tid] = (n < N) ? d_x[n] : 0.0;

	//同步，保证所有线程块的共享内存都准备就绪
    __syncthreads();

    for (int offset = blockDim.x >> 1;offset > 0;offset>>=1)
    {

        if (tid < offset)// 迭代到后期会发生 warp divergence
        {
            s_y[tid] += s_y[tid + offset];
        }
        __syncthreads();
    }

	//因为共享内存变量的生命周期仅仅在核函数内
	//所以必须在核函数结束之前将共享内存中的某些结果保存到全局内存
	//当 bid 等于0时，将第0个线程块中的s_y[0] 副本复制给 d_y[0]
	//当 bid 等于1时，将第1个线程块中的s_y[0] 副本复制给 d_y[1]
	
    if (tid == 0)
    {
        d_y[bid] = s_y[0];
    }
}

```

一般来说，在核函数中对共享内存访问的次数越多，则由使用共享内存带来的加速效果越明显。

在我们的数组归约问题中，使用共享内存相对于仅使用全局内存还有两个好处：一个是不再要求全局内存数组的长度 N 是线程块大小的整数倍，另一个是在规约的过程中不会改变全局内存数组中的数据（在仅使用全局内存时，数组 d_x 中的部分元素被改变）。这两点在实际的应用中往往都是很重要的。

共享内存的另一个作用是改善全局内存的访问方式（将非合并的全局内存访问转化为合并的）。

> [!NOTE] 动态共享内存
>
> 我们可以将前一个版本的静态共享内存改成动态共享内存，只需要做以下两处修改：
> ```cpp
> //1.在调用核函数的执行配置中写下第三个参数：
> //第三个参数就是核函数中每个线程块需要定义的动态共享内存的字节数
> //不写默认为0
> <<<grid_size, block_size, sizeof(real) * block_size>>>
> //2. 要使用动态共享内存，还需要改变核函数中共享内存变量的声明方式
> //用extern修饰，不要指定大小
> extern __shared__ real s_y[];
> //extern __shared__ real *s_y; 错误，无法通过编译，指针!=数组
> ```

无论使用什么 GPU 进行测试，使用动态共享内存的核函数和使用静态共享内存的核函数在执行时间上几乎没有差别。所以，使用动态共享内存不会影响程序性能，但有时可提高程序的可维护性。

### 矩阵转置-共享内存

如果不利用共享内存的话，在[矩阵转置](13-e-1%20(全局内存）.md#矩阵转置[%201])问题中，对全局内存的读和写这两个操作，总有一个是合并的，另一个是非合并的。

```cpp
__global__ void transpose1(const real *A, real *B, const int N)
{
	//TILE_DIM = 32
    __shared__ real S[TILE_DIM][TILE_DIM];
    int bx = blockIdx.x * TILE_DIM;
    int by = blockIdx.y * TILE_DIM;

    int nx1 = bx + threadIdx.x;
    int ny1 = by + threadIdx.y;
    if (nx1 < N && ny1 < N)
    {
        S[threadIdx.y][threadIdx.x] = A[ny1 * N + nx1];
    }
	//一般来说在利用共享内存中的数据之前，都要进行线程块内的同步操作      //从而确保共享内存数组中的所有元素都已经更新完毕。
    __syncthreads();

    int nx2 = bx + threadIdx.y;
    int ny2 = by + threadIdx.x;
    if (nx2 < N && ny2 < N)
    {
        B[nx2 * N + ny2] = S[threadIdx.x][threadIdx.y];
    }
}
```

其中，`S[threadIdx.y][threadIdx.x] = A[ny1 * N + nx1]`对全局内存的访问是合并的，相邻的 `threadIdx.x`与全局内存中相邻的数据对应。

`B[nx2 * N + ny2] = S[threadIdx.x][threadIdx.y]`对全局内存的访问是合并的，相邻的 `threadIdx.x`与全局内存B中相邻的数据对应。

但是这个核函数仍有优化的空间，那就要引入一个新概念——bank。

### bank-共享内存访问

关于共享内存，有一个内存 bank 的概念值得注意。为了获得高的内存带宽，共享内存在物理上被分为 32 个（刚好等于一个线程束中的线程数目，即内建变量 warpSize 的值） 同样宽度的、能被同时访问的内存 bank。

我们可以将 32个bank从0到31编号。在每一个 bank 中，又可以对其中的内存地址从 0 开始编号。为方便起见，我们将所有 bank 中编号为 0 的内存称为第一层内存；将所有 bank 中编号为 1 的内存称为第二层内存。每个 bank 的宽度为 4 字节。(不关注开普勒架构。)

对于 bank 宽度为 4 字节的架构，共享内存数组是按如下方式线性地映射到内存 bank 的： 共享内存数组中连续的 128 字节的内容分摊到 32 个 bank 的某一层中，每个 bank 负责 4 字节的内容。

例如，对一个长度为 128 的单精度浮点数变量的共享内存数组来说，第 0-31 个数组元素依次对应到 32 个 bank 的第一层，第 32-63 个数组元素依次对应到 32 个 bank 的二层，最后第 96-127 个数组元素依次对应到 32 个 bank 的第四层。也就是说，每个 bank 分摊了 4 个在地址上相差 128 字节的数据


![](../../../../../files/images/MLsys/13-a/13-e-4.png)

只要同一线程束内的多个线程不同时访问同一个 bank 中不同层的数据，该线程束对共享内存的访问就只需要一次内存事务（memory transaction）。当同一线程束内的多个线程试图访问同一个 bank 中不同层的数据时，就会发生 bank 冲突。

> [!question] 为什么访问同一个Bank的同一个地址不冲突？
> 这是硬件的一个重要优化，称为**广播（Broadcast）**或**多播（Multicast）**。
> - 当硬件检测到多个线程请求的**Bank号相同**，并且**Bank内的地址也完全相同时**，它会识别出这是一个“所有人都想要同一份数据”的场景。
> - 此时，Bank只需要执行**一次读取操作**。然后，它会将读取到的这份数据**同时广播**给所有发出请求的线程。这个过程可以在**一个时钟周期**内完成，因此**没有性能损失**。

在一个线程束内对同一个 bank 中的 n 层数据同时访问将导致 n 次内存事务，称为发生了 n 路 bank 冲突。 最坏的情况是线程束内的 32 个线程同时访问同一个 bank 中 32 个不同层的地址，这将导致 32 路 bank 冲突。这种 n 很大的 bank 冲突是要尽量避免的。

> [!question] 为什么？
> - **硬件：** Bank这个物理单元的硬件设计决定了它**在一个时钟周期内只能处理一个地址请求**。它无法同时从addr1和addr2读取数据。
> - **序列化（Serialization）：** 为了解决这个冲突，硬件别无选择，只能将这些请求**序列化**。
>     1. 在第一个时钟周期，Bank处理线程A的请求，返回数据。线程B的请求被阻塞。
>     2. 在第二个时钟周期，Bank处理线程B的请求，返回数据。
> - **性能下降：** 原本希望在1个周期内完成的并行访问，现在需要2个周期。如果有 k 个线程冲突在同一个Bank上，就需要 k 个周期。这就是性能下降的直接原因。

所以可以看出，原来代码中的`B[nx2 * N + ny2] = S[threadIdx.x][threadIdx.y]`对共享内存的访问是正好引发了32路bank冲突的，正如如图图8.1上所显现的。

所以，通常可以用改变共享内存数组大小的方式来消除或减轻共享内存的 bank 冲突。例如， 将上述核函数中的共享内存定义修改为如下：`__shared__ real S[TILE_DIM][TILE_DIM + 1]`，同一个线程束中的 32 个线程（连续的 32 个 threadIdx.x 值）将对应共享内存 数组 S 中跨度为 33 的数据