### CUDA核函数的映射

![](../../../../../files/images/MLsys/13-b/13-b-2-19.png)

一个SM（流多处理器）可以同时运行多个Block，这些Block是并行执行的。SM中的调度器会在不同的Block之间切换，以最大化硬件利用率并隐藏延迟。当一个Block等待内存访问或其他操作时，调度器可以快速切换到另一个Block，使其继续执行。
![](../../../../../files/images/MLsys/13-a/13-a-9-1.png)

![](../../../../../files/images/MLsys/13-a/13-a-9-5.png)

线程映射：每个线程都是一个SIMD向量通道（SIMD vector lane）
线程组（Warps）映射: SIMD指令作用于Warp，Warp宽度为32个元素，为逻辑SIMD宽度
线程块（Thread blocks）映射：每个线程块都调度在一个SM上运行，为达到SM的峰值效率，需要每个SM同时运行多个线程块。

![](../../../../../files/images/MLsys/13-b/13-b-2-20.png)

- **标号 ①**: 这条线从线程块 (Block) 指向一个SM表明：**一个线程块会被调度到一整个 SM上去执行**。运行在这个 SM 上的所有核心将共同执行这个块内的所有线程。这个块所使用的共享内存（Per-block Shared Memory）也自然地映射到了这个 SM 内部的物理 SMem 上。
- **标号 ②**: 这条线从线程 (Thread) 指向 SM 内部的核心 (Cores)。这说明**一个线程中的具体指令，最终是在 SM 内部的某个核心上执行的**。线程的私有变量（myVar）会被加载到这些核心的寄存器中进行计算。

![](../../../../../files/images/MLsys/13-b/13-b-2-21.png)

> [!NOTE] 解释
> - `float myVar` -> Registers / Per-thread Local Memory (寄存器 / 每线程局部内存)
>     - 概念: 这是定义在线程内部的普通变量，生命周期和作用域都仅限于该线程。
>     - 物理存储:
>         - Registers (寄存器): 这是最快的存储。编译器会优先将这些变量放入 GPU 核心（Core）的寄存器中。
>         - Per-thread Local Memory : 如果寄存器不够用（例如变量太多、或者定义了无法放入寄存器的大数组），**变量就会被“溢出”到局部内存中**。需要注意的是，局部内存虽然在逻辑上是线程私有的，但在物理上位于较慢的设备内存（Device Memory）中。
>     - 执行单位: Thread (线程) 是执行这些私有变量操作的基本单位。
> 	
> - `__shared__ float shVar` -> Per-block Shared Memory
>     - 概念: 使用 `__shared__` 关键字声明的变量。它对于**同一个线程块（Block）**内的所有线程都是可见和可访问的。
>     - 物理存储: 存储在非常快速的**片上共享内存（On-chip Shared Memory）**中。它的速度远快于全局内存，几乎和寄存器一样快。这是实现块内线程高效通信和数据交换的关键。
>     - 执行单位: **Block (线程块)** 是共享内存的作用域。一个线程块内的所有线程共享同一块共享内存。
> 	
> - `__device__ float glVar` -> Per-app Device Global Memory 
>     - 概念: 使用 `__device__` 关键字声明的变量（或在核函数外、设备代码中定义的变量）。它对于本次应用（Grid）中的**所有线程块、所有线程**都是可见和可访问的。
>     - 物理存储: 存储在 GPU 的**设备内存（Device Memory）**中，通常是DRAM。这是最大但也是最慢的内存。CPU和GPU之间传输的数据，以及核函数的输入输出数据，通常都存放在这里。
>     - 作用域: 整个应用程序（Grid）。

### Streaming Multiprocessor

![](../../../../../files/images/MLsys/13-a/13-a-9-2.png)

![](../../../../../files/images/MLsys/13-a/13-a-9-4.png)


> [!question] 为什么 Warp 具有零开销上下文切换？
> - 上下文 (Context) 是什么？: 一个线程的“上下文”就是它在任何时刻的完整状态，主要包括：
>     1. 程序计数器 (Program Counter, PC): 指示下一条要执行的指令地址。
>     2. 寄存器 (Registers): 存储该线程的所有变量和中间计算结果。
> - CPU的昂贵上下文切换:
>     - 当CPU需要从线程A切换到线程B时，由于CPU的寄存器数量**有限**，它必须执行一个由**操作系统**主导的、缓慢的过程：
>         1. 保存 (Save): 将线程A的所有寄存器内容**写入到内存**（通常是push寄存器到线程的内核栈）中。
>         2. 加载 (Load): 将线程B之前保存的上下文**从内存中读回到寄存器**（pop回寄存器）里。
>     - 内存读写的过程涉及到几百甚至上千个时钟周期。
> - GPU的“零开销”上下文切换:
>     - GPU采用了空间换时间的设计。一个SM拥有一个**巨大无比的寄存器文件 (Register File)**。
>     - 这个Register File被**物理地划分**成许多块，**足够同时存放所有驻留在该SM上的Warps的所有线程的完整上下文**。
>     -  当Warp A在等待内存，调度器决定切换到Warp B时，**不需要进行任何内存读写**。Warp B的上下文**已经**原封不动地在属于它的那片硬件寄存器里了。
>     - **硬件调度器 (HW) 需要做的唯一一件事**，就是改变一个内部的**指针**，让指令分发单元开始从指向Warp B的寄存器区域和程序计数器获取指令和数据。
>     - 这个切换指针的操作只需要一两个时钟周期，与访存延迟相比，其开销可以忽略不计，因此被称为零开销。


下面则是单个SM的简化图：

1. 控制流是统一的: 一个Warp Scheduler只发出**一条指令**。
2. 数据流是并行的: 这条指令被多个执行单元同时执行，每个单元处理自己的数据。
3. **通过大量Warp切换来隐藏延迟**: Warp Scheduler是实现这一点的关键。
4. **专门的硬件来优化内存访问**: Address coalescing unit是提升内存带宽利用率的核心部件。

![](../../../../../files/images/MLsys/13-b/13-b-2-22.png)

> [!NOTE] SM解释
> 指令前端 (Instruction Front-End)  控制单元负责取指令和调度，但不负责计算。
> - Instruction Cache (指令缓存):
>     - 功能: 存储最近使用过的指令。当需要执行某条指令时，首先在这里查找。
> - Warp Scheduler (Warp 调度器):
>     - 功能: 这是核心的控制单元。一个SM上通常会同时驻留多个Warp。调度器的任务是：
>         1. 选择Warp: 从所有“准备就绪”（没有在等待内存访问等）的Warp中，挑选一个出来执行。
>         2. 选择指令: 从被选中的Warp中取出下一条要执行的指令。
>     - 关键作用: 通过在多个Warp之间快速切换，可以有效地**隐藏延迟**。例如，当WarpA在等待读取数据时（这个过程很慢），调度器可以立刻切换到WarpB去执行计算指令，从而让计算单元始终保持忙碌。
> - Instruction Register (指令寄存器):
>     - 功能: 一个临时的存储单元，用于存放Warp调度器刚刚选出的、即将要被执行的那条**单一指令**。
> 
> 执行核心 (Execution Core) 
> - SIMD lanes (thread processors) (SIMD通道 / 线程处理器):
>     - 每一个通道都是一个简单的处理单元，可以看作一个**CUDA Core**
>     - 工作方式: Instruction Register 中的那一条指令会被**广播**给所有这16个SIMD lane。然后，这16个lane**在同一时刻、执行完全相同的操作**。
>     - 例如，如果指令是 ADD R1, R2，那么所有16个lane都会同时执行加法操作。但是，每个lane操作的是**自己的寄存器**中的数据（Lane 0用它自己的R1和R2，Lane 1用它自己的R1和R2，等等）。这就是SIMD的体现。
> - Registers (Reg) (寄存器):
>     - 结构: 每个SIMD lane都拥有自己的一大组私有寄存器。图中标注为 1K x 32，表示每个lane有1024个32位的寄存器。
>     - 功能: 存放对应线程的私有变量（比如上一张图中的 myVar）。这是GPU上最快的存储。
> - Load store unit (加载/存储单元):
>     - 功能: 每个lane都配有一个，专门负责处理内存读写操作。当指令是加载（从内存读）或存储（写回内存）时，这个单元就会被激活。
>         
> 内存系统 (Memory System)
> 这部分负责连接执行核心和不同层级的内存。
> - Address coalescing unit (地址合并单元):
>     - 功能: 这是一个非常重要的优化单元。当一个Warp中的多个线程（例如，这16个线程）同时访问内存时，它们会各自产生一个内存地址。如果这些地址是连续的或有规律的（例如，线程i访问地址base + i），这个单元可以将这16个零散的请求**合并成一个或几个大的、整块的内存访问请求，从而提高效率**。
> - Interconnection network (互联网络):
>     - 功能: 一个内部的交换网络，负责将加载/存储单元的请求路由到正确的内存位置（本地内存或全局内存），并将数据返回。
> - Local memory (64 KB) (本地内存):
>     - **功能**: 这就是上一张图中提到的**片上共享内存 (On-chip Shared Memory / SMem)**。它速度非常快，被这个SM内的所有SIMD lane共享。用于实现Warp内或同一个线程块内线程之间的高效通信。
> - To global memory (到全局内存):
>     - 功能: 连接到SM外部的、更大但更慢的GPU显存（Device Memory）。

当然，现代nvidia内部的cuda core远超16个。

一个 SM一般包含如下资源：
 - 一定数量的寄存器。 
 - 一定数量的共享内存。 
 - 常量内存的缓存。 
 - 纹理和表面内存的缓存。  
 - L1 缓存。  
 -  4 个线程束调度器（warp scheduler），用于在不同线程的上下文之间迅速地切换，以及为准备就绪的线程束发出执行指令。 
 - 执行核心，包括： 
	 - 若干整型数运算的核心（INT32）。 
	 - 若干单精度浮点数运算的核心（FP32）。
	 - 若干双精度浮点数运算的核心（FP64）。 
	 - 若干单精度浮点数超越函数（transcendental functions）的特殊函数单元（Special Function Units，SFUs）。 
	 - 若干混合精度的张量核心（tensor cores）。

Hopper架构的SM被划分为了4个主要的，功能相同的子分区。一个SM在硬件层面就可以实现四路并行。每个处理块都拥有自己独立的调度器和执行单元。

![hopper](https://developer-blogs.nvidia.com/wp-content/uploads/2022/03/H100-Streaming-Multiprocessor-SM.png)