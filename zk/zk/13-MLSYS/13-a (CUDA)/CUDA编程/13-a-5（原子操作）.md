# 原子操作

在 CUDA 中，一个线程的原子操作可以在不受其他线程的任何操作的影响下完成对某个（全局内存或共享内存中的）数据的一套**读-改-写**操作。 该套操作也可以说是不可分的。

在前几个版本的数组归约函数中，核函数并没有做全部的计算，而只是将一个长一些的数组 d_x 变成了一个短一些的数组 d_y，后者中的每个元素为前者中若干元素的和。 

在调用核函数之后，将短一些的数组复制到主机，然后在主机中完成了余下的求和。如果能在 GPU 中计算出最终结果，则有望显著地减少整体的计算时间，提升程序性能。

有两种方法能够在 GPU 中得到最终结果，一是用另一个核函数将较短的数组进一步归约，得到最终的结果（一个数值）；二是在先前的核函数的末尾利用原子函数进行归约，直接得到最终结果。

如果我们将代码更改如下：

```cpp
if (tid == 0) 
{ 
	// 原来是 d_y[bid] = s_y[0];
	d_y[bid] += s_y[0]; 
}
```

该语句事实上不能实现我们的目的。该语句在每一个线程块的第 0 号线程都会被执行， 但是它们**执行的次序是不确定的**。

在每一个线程中，该语句其实可以分解为两个操作：首先从 d_y[0] 中取数据并与 s_y[0] 相加，然后将结果写入 d_y[0]。

不管次序如何，只有当一个线程的**读-写**操作不被其他线程干扰时，才能得到正确的结果。如果一个线程还未将结果写入d_y[0]，另一个线程就读取了d_y[0]，那么这两个线程读取的 d_y[0] 就是一样的，这必将导致错误的结果。

要正确得到所有线程块中的 s_y[0] 的和，必须使用原子函数:

```cpp
if (tid == 0) 
{ 
	atomicAdd(&d_y[0], s_y[0]); 
}
```

原子函数`atomicAdd(address, val)`的第一个参数是待累加变量的地址 `address`，第二个参数是累加的值 `val`。该函数的作用是将地址 `address`中的旧值old读出，计算 old + val， 然后将计算的值存入地址 address。

这些操作在一次原子事务（atomic transaction）中完成， 不会被别的线程中的原子操作所干扰。原子函数不能保证各个线程的执行具有特定的次序， 但是能够保证每个线程的操作一气呵成，不被其他线程干扰。

原子函数`atomicAdd`其实是有返回值的，只不过在上述程序中没有使用。

### 原子函数

原子函数对它的第一个参数指向的数据进行一次**读-改-写**的原子操作。第一个参数可以指向全局内存，也可以指向共享内存。对所有参与的线程来说，原子操作是一个线程一个线程轮流做的，但没有明确的次序。 另外，原子函数没有同步功能。

下面，我们列出所有原子函数的原型。[^1]我们约定，对每一个线程来说，address 所指变量的值在实施与该线程对应的原子函数前为 old，在实施与该线程对应的原子函数后为 new。对每一个原子函数来说，返回值都是 old。另外要注意的是，这里介绍的原子函数都是 `__device__` 函数，只能在核函数中使用。


![](../../../../../files/images/MLsys/13-a-65-1.png)

在所有原子函数中，atomicCAS 函数是比较特殊的：所有其他原子函数都可以用它实现。

> [!question] 为什么都可以用CAS函数实现？
> 所有其他的原子操作（如 Add, Min, Max, Exchange）都遵循一个共同的模式，即**读取-修改-写入**。多线程环境下，这会产生一个经典的竞态条件
> atomicCAS 完美地解决了这个问题。它通过**乐观锁**和**重试循环**的模式，保证了**读取-修改-写入**操作的原子性。
> ```cpp
> int atomicAdd(int* address, int value_to_add) {
>    int old_value;
>    int new_value;
>    do {
>        old_value = *address; 
>        new_value = old_value + value_to_add;
>    } 
>    while (atomicCAS(address, old_value, new_value) != old_value);
>return old_value;
>}
>```
>
> atomicCAS 如果成功，会返回它操作前的值，这个值正好等于我们传入的 old_value，于是 while 条件 (returned_value != old_value) 为假，循环退出。
   如果在 old_value = *address 和 atomicCAS(...) 之间，有另一个线程修改了 address 的值，那么 atomicCAS 执行时会发现 *address 的值不等于 old_value，于是写入失败。此时 atomicCAS 返回的将是那个被其他线程修改后的**新值**。这个返回值不等于我们期望的 old_value，于是 while 条件为真，循环继续，用这个最新的值开始新一轮的读-改-写尝试。

### 示例[^2][^3]

```cpp
void __global__ find_neighbor_atomic
(
    int *d_NN, int *d_NL, const real *d_x, const real *d_y,
    const int N, const real cutoff_square
)
{
    const int n1 = blockIdx.x * blockDim.x + threadIdx.x;
    if (n1 < N)
    {
        const real x1 = d_x[n1];
        const real y1 = d_y[n1];
        for (int n2 = n1 + 1; n2 < N; ++n2)
        {
            const real x12 = d_x[n2] - x1;
            const real y12 = d_y[n2] - y1;
            const real distance_square = x12*x12 + y12*y12;
            if (distance_square < cutoff_square)
            {
                d_NL[n1 * MN + atomicAdd(&d_NN[n1], 1)] = n2;
                d_NL[n2 * MN + atomicAdd(&d_NN[n2], 1)] = n1;
            }
        }
    }
}
```

> [!question] 改写
> 在使用原子函数时还有一个容易犯的错误，就是没有合理利用返回值。
> 如果将使用原子函数的代码:
> ```cpp
>d_NL[n1 * MN + atomicAdd(&d_NN[n1], 1)] = n2;
>d_NL[n2 * MN + atomicAdd(&d_NN[n2], 1)] = n1； 
>//改写为如下代码
>d_NL[n1 * MN + d_NN[n1]] = n2;
>atomicAdd(&d_NN[n1], 1); 
>d_NL[n2 * MN + d_NN[n2]] = n1; 
>atomicAdd(&d_NN[n2], 1);
>```
>那就改写错误了，这是因为，第二行的原子函数读取的 `d_NN[n1]` 的值并不能保证是改写后第一行使用的 `d_NN[n1]` 的值，而有可能**是被别的线程的原子函数改动过的值**。这是在使用原子函数所访问变量的“旧值”时需要特别注意的一点。
>
>下面是一种正确的写法：
>```cpp
>int tmp1 = atomicAdd(&d_NN[n1], 1); 
>d_NL[n1 * MN + tmp1] = n2; 
>int tmp2 = atomicAdd(&d_NN[n2], 1); 
>d_NL[n2 * MN + tmp2] = n1;
>```

如下还有不用原子操作实现相同功能的版本：
```cpp
void __global__ find_neighbor_no_atomic
(
    int *d_NN, int *d_NL, const real *d_x, const real *d_y,
    const int N, const real cutoff_square
)
{
    const int n1 = blockIdx.x * blockDim.x + threadIdx.x;
    if (n1 < N)
    {
        int count = 0;
        const real x1 = d_x[n1];
        const real y1 = d_y[n1];
        for (int n2 = 0; n2 < N; ++n2)
        {
            const real x12 = d_x[n2] - x1;
            const real y12 = d_y[n2] - y1;
            const real distance_square =x12*x12 + y12*y12;
            if ((distance_square<cutoff_square)&&(n2!= n1))
            {
                d_NL[(count++) * N + n1] = n2;
            }
        }
        d_NN[n1] = count;//优化
    }
}  
```

- 一个值得一提的优化策略是用一个寄存器变量 count 减少了对全局内存变 量 `d_NN[n1]` 的访问，原先需要在`d_NL[n1 * MN + atomicAdd(&d_NN[n1], 1)] = n2;`中做多次原子操作以及访问全局内存，改动过后用`count`变量记录数量，只需要一次全局内存访问。

- 代码也改变了邻居列表中的数据排列方式：将 `d_NL[n1 * MN + count++]` 改成 了 `d_NL[(count++) * N + n1]`。因为 n1 的变化步调与 threadIdx.x 一致，这样修改之后，对全局内存数组 d_NL 的访问将是合并的




[^1]: [CUDA 编程：基础与实践_樊哲勇, 页面 107](files/books/MLSys/CUDA%20编程：基础与实践_樊哲勇.pdf#page=107&selection=104,2,104,11)
[^2]: [CUDA 编程：基础与实践_樊哲勇, 页面 111](files/books/MLSys/CUDA%20编程：基础与实践_樊哲勇.pdf#page=111&selection=215,2,215,4)
[^3]: https://github.com/brucefan1983/CUDA-Programming/blob/master/src/09-atomic/neighbor2gpu.cu#L202
